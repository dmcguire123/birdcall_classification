{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Introduction](#1)  \n",
    "2. [Exploratory Data Analysis](#2)  \n",
    "    * [Importing Libraries](#21)\n",
    "    * [Load Bird Species Dataset](#22)\n",
    "    * [Bird Species Analysis](#23)\n",
    "    * [Recordings by geographical location](#231)\n",
    "    * [Samples by Country](#24)\n",
    "    * [Samples by Date](#25)\n",
    "    * [Birds Seen](#26)\n",
    "    * [Pitch](#27)\n",
    "    * [Sampling Rate](#28)\n",
    "    * [Volume](#29)\n",
    "    * [Channels](#210)\n",
    "    * [Recordists](#211)\n",
    "    * [Ratings](#212)\n",
    "    * [Bird seen by Country](#213)\n",
    "3. [Audio Data analysis](#3)   \n",
    "     * [Playing audio](#31)\n",
    "     * [Visualizing audio in 2D](#32)\n",
    "     * [Spectrogram analysis](#33)\n",
    "4. [Feature Extraction](#4)    \n",
    "     * [Spectral Centroid](#41)\n",
    "     * [Spectral Bandwidth](#42)\n",
    "     * [Spectral Rolloff](#43)\n",
    "     * [Zero-Crossing Rate](#44)\n",
    "     * [Mel-Frequency Cepstral Coefficients(MFCCs)](#45)\n",
    "     * [Chroma feature](#46)\n",
    "5. [Compare sound features](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Introduction<a id=\"1\"></a> <br>\n",
    "\n",
    "Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. \n",
    "\n",
    "With proper sound detection and classification, researchers could automatically intuit factors about an area’s quality of life based on a changing bird population.\n",
    "\n",
    "There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar.\n",
    "\n",
    "## Objective  \n",
    "To identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings.\n",
    "\n",
    "So let us use the dataset of <b> Cornell Lab of Ornithology’s Center for Conservation Bioacoustics (CCB)</b> to do a complete exploratory data analysis and finding the insights about data and based on the findings come up with AI model that can achieve the above objective.\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis<a id='2'></a>\n",
    "## <font size='4' color='blue'>Importing Libraries</font><a id='21'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install pandas-bokeh\n",
    "!pip install chart_studio\n",
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import plotly.express as px\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import librosa\n",
    "import warnings\n",
    "import IPython\n",
    "import os\n",
    "import wave\n",
    "import pandas_profiling \n",
    "import pandas_bokeh\n",
    "from bokeh.models.widgets import DataTable, TableColumn\n",
    "from bokeh.models import ColumnDataSource\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import iplot\n",
    "import cufflinks\n",
    "from IPython.display import IFrame\n",
    "from tqdm import tqdm_notebook\n",
    "import IPython as ipy\n",
    "import IPython.display as ipyd\n",
    "import folium\n",
    "from folium.plugins import HeatMap, HeatMapWithTime\n",
    "import plotly.express as px\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile as wav\n",
    "import struct\n",
    "from scipy.io import wavfile as wav\n",
    "from colorama import Fore, Back, Style\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import datetime as dt\n",
    "from datetime import datetime   \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.show()\n",
    "pandas_bokeh.output_notebook()\n",
    "pd.set_option('plotting.backend', 'pandas_bokeh')\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Load Bird songs Dataset</font><a id='22'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/birdsong-recognition/train.csv\")\n",
    "media_path = '/kaggle/input/birdsong-recognition/train_audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def bird_sound_plotter(full_path,data):   \n",
    "    rate, wave_sample = wav.read(full_path)\n",
    "    wave_file = open(full_path,\"rb\")\n",
    "    riff_fmt = wave_file.read(36)\n",
    "    bit_depth_string = riff_fmt[-2:]\n",
    "    bit_depth = struct.unpack(\"H\",bit_depth_string)[0]\n",
    "    print(Fore.CYAN+data['title'].upper())\n",
    "    print('_'*len(data['title']))\n",
    "    print('')\n",
    "    print('Scientific Name:',data['sci_name'])\n",
    "    print(\"Recorded in {} country \".format(data['country']))\n",
    "    print('Recordist: ',data['author'])\n",
    "    print('Number of Channels: ',wave_sample.shape[1] if len(wave_sample.shape)>1 else 1)\n",
    "    print('Number of Samples: ',len(wave_sample))\n",
    "    print('Rating: ',data['rating'])\n",
    "    print('Sampling rate: ',rate,'Hz')\n",
    "    print('Bit depth: ',bit_depth)\n",
    "    print('Duration: ',wave_sample.shape[0]/rate,' second')\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(wave_sample)\n",
    "    return ipd.Audio(full_path)\n",
    "\n",
    "def plot_bird_sound_wave(sp):\n",
    "    train_sound_data = train[train['species']==sp]\n",
    "    idx = np.random.choice(train_sound_data.index,1)[0]\n",
    "    bird_sound_data = train_sound_data.loc[idx,:]\n",
    "    src = os.path.join('/kaggle/input/birdsong-recognition/train_audio/',bird_sound_data['ebird_code'],bird_sound_data['filename'])\n",
    "    bird_sound_mp3 = AudioSegment.from_mp3(src)\n",
    "    filename=bird_sound_data['filename'].split('.')[0]+'.wav'\n",
    "    bird_sound_mp3.export(filename,format='wav')\n",
    "    return bird_sound_plotter(filename,bird_sound_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Bird Species Analysis</font><a id='23'></a>\n",
    "\n",
    "Let us find out from the dataset how many bird species exist and what are they"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebird = train['ebird_code'].value_counts().index.to_list()\n",
    "ebird_code_path = 'https://ebird.org/species/'\n",
    "species = [ebird_code_path+i for i in ebird]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(species[16], width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(species[100], width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} unique species of birds in train dataset\".format(train.species.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['species'].value_counts().sort_values(ascending = False).iplot(kind='bar',color='#85500BF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Recordings by Geographical Location</font><a id='231'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "bird_song_geographical_location = user_secrets.get_secret(\"bird_song\")\n",
    "bird_song_map = user_secrets.get_secret(\"location\")\n",
    "train.latitude = train.latitude.str.replace('Not specified','nan').astype(np.float16)\n",
    "train.longitude = train.longitude.str.replace('Not specified','nan').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.set_mapbox_access_token(bird_song_map)\n",
    "fig = px.scatter_mapbox(train,\n",
    "                lat='latitude',\n",
    "                lon='longitude',\n",
    "                size='duration',\n",
    "                color='rating',\n",
    "                hover_name='species',\n",
    "                hover_data=['country','elevation','duration'],\n",
    "                color_continuous_scale=px.colors.sequential.Rainbow,\n",
    "                mapbox_style='stamen-terrain',\n",
    "                zoom=0.5)\n",
    "fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "fig.update_geos(projection_type=\"mercator\")\n",
    "fig.update_layout(height=1000,width=1200,margin={\"r\":100,\"t\":200,\"l\":0,\"b\":0})\n",
    "fig.update_layout(title='<b>Recording Locations</b>',template='seaborn',\n",
    "                  hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Samples by Country</font><a id='24'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "country_counts = train.country.value_counts().sort_index(ascending=False) \n",
    "df1 = pd.DataFrame({\"Count\": country_counts},index=train.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "p_hbar = df1.plot_bokeh(\n",
    "    kind=\"barh\",\n",
    "    x=country_counts.index,\n",
    "    xlabel=\"Count\",\n",
    "    ylabel=\"Country\",\n",
    "    title=\"Distribution of Bird Species Across Country\", \n",
    "    alpha=0.4,\n",
    "    figsize=(800,300),\n",
    "    legend = \"top_right\",\n",
    "    show_figure=False)\n",
    "pandas_bokeh.plot_grid([[p_hbar]],plot_width=1100,plot_height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Date of Recordings</font><a id='25'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100, 120))\n",
    "train['date'].value_counts().sort_index().plot(color='blue',alpha=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Birds Seen</font><a id='26'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bird_seen'].value_counts()\n",
    "\n",
    "labels = train['bird_seen'].value_counts().index\n",
    "values = train['bird_seen'].value_counts().values\n",
    "colors=['#9793bf','#bf3fbf']\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n",
    "                             insidetextorientation='radial',marker=dict(colors=colors))])\n",
    "fig.update_layout(title='Bird Seen')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Pitch</font><a id='27'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pitch'].value_counts()\n",
    "labels = train['pitch'].value_counts().index\n",
    "values = train['pitch'].value_counts().values\n",
    "colors=['#0093bf','#af3fbf']\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n",
    "                             insidetextorientation='radial',marker=dict(colors=colors))])\n",
    "fig.update_layout(title='Pitch',annotations=[dict(text='Pitch', x=0.51, y=0.5, font_size=20, showarrow=False)])\n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n",
    "fig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n",
    "                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Sampling Rate</font><a id='28'></a>\n",
    "\n",
    "Sampling rate (audio) or sampling frequency defines the number of samples per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sampling_rate'].value_counts().sort_values(ascending = False).iplot(kind='bar',color='#09055BF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Volume</font><a id='29'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['volume'].value_counts()\n",
    "labels = train['volume'].value_counts().index\n",
    "values = train['volume'].value_counts().values\n",
    "colors=['#9993bf','#df3abf']\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n",
    "                             insidetextorientation='radial',marker=dict(colors=colors))])\n",
    "fig.update_layout(title='Pitch',annotations=[dict(text='Volume', x=0.51, y=0.5, font_size=18, showarrow=False)])\n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n",
    "fig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n",
    "                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Channels</font><a id='210'></a>\n",
    "Channel is the passage way a signal or data is transported.One Channel is usually referred to as mono, while more Channels could either indicate stereo, surround sound and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = train.groupby('channels',as_index=False)['title'].count().sort_values('channels')\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=channels['channels'],y=channels['title'],marker_line_color='red',marker_line_width=2.5,text=channels['title'],textposition='auto'))\n",
    "fig.update_layout(template='seaborn',height=600,title='Channels',paper_bgcolor='rgb(255,255,255)',plot_bgcolor='rgb(255,255,255)',\n",
    "                 xaxis=dict(title='Channels',nticks=20,mirror=True,linewidth=1,linecolor='green'),\n",
    "                 yaxis=dict(title='Counts',mirror=False,linewidth=1,linecolor='black',gridcolor='darkgrey'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Recordist</font><a id='211'></a>\n",
    "Let us find out the number of people who provided the recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['recordist'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets say view top 25 recordists and their contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['recordist'].value_counts()[:25].sort_values().iplot(kind='barh',color='#89000BF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Ratings</font><a id='212'></a>\n",
    "Let us find out the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = train.groupby('rating',as_index=False)['title'].count().sort_values('rating')\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=ratings['rating'],y=ratings['title'],marker_line_color='red',marker_line_width=2.5,text=ratings['title'],textposition='auto'))\n",
    "fig.update_layout(template='seaborn',height=600,title='Ratings',paper_bgcolor='rgb(255,255,255)',plot_bgcolor='rgb(255,255,255)',\n",
    "                 xaxis=dict(title='Ratings',nticks=20,mirror=True,linewidth=1,linecolor='green'),\n",
    "                 yaxis=dict(title='Counts',mirror=False,linewidth=1,linecolor='black',gridcolor='darkgrey'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Bird Seen by Country</font><a id='213'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_seen_countries = train.groupby(['country','bird_seen'],as_index=False).agg({'title':'count','rating':'mean'})\\\n",
    "    .sort_values('title',ascending=False).reset_index()\n",
    "bird_seen_countries = bird_seen_countries.loc[:50,:]\n",
    "seen_color = {'yes':'rgb(130, 17, 193)','no':'rgb(229, 58, 156)'}\n",
    "fig = go.Figure()\n",
    "for seen in ['yes','no']:\n",
    "    fig.add_trace(go.Bar(name=seen,y=bird_seen_countries[bird_seen_countries['bird_seen']==seen]['country'],\n",
    "                         x=bird_seen_countries[bird_seen_countries['bird_seen']==seen]['title'],orientation='h',\n",
    "                         marker_line_color='black',marker_line_width=1.5,\n",
    "                         text=np.round(bird_seen_countries[bird_seen_countries['bird_seen']==seen]['rating'],2),textposition='inside',\n",
    "                         marker_color=seen_color[seen]))\n",
    "fig.update_layout(height=1500,template='seaborn',paper_bgcolor='rgb(255,255,255)',plot_bgcolor='rgb(255,255,255)',barmode='stack',\n",
    "                  hovermode='y unified',width=1200,\n",
    "                 xaxis=dict(title='No of Recordings',type='log',mirror='allticks',linewidth=2,linecolor='black',\n",
    "                            showgrid=True,gridcolor='darkgray'),\n",
    "                 yaxis=dict(title='Country',mirror=True,linewidth=2,linecolor='black',tickfont=dict(size=12)),\n",
    "                 legend=dict(title='<b>Bird seen in Country</b>',x=0.71,y=0.95,bgcolor='rgba(255, 255, 255, 0)',\n",
    "                             bordercolor='rgba(255, 255, 255, 0)'),\n",
    "                 title='<b>Number of Recordings & Average Ratings per Country [Top 50]</b>')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Data Analysis<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Playing Audio</font><a id='31'></a>\n",
    "\n",
    "There are about 264 bird species in the dataset and for each species multiple recordings are present.\n",
    "We will be demonstrating the randomly bird chirps recording from the dataset and its sound plot.\n",
    "\n",
    "### Snow Bunting\n",
    "![](https://res-2.cloudinary.com/ebirdr/image/upload/s--GEPz7XJt--/f_auto,q_auto,t_full/2463-snow-bunting.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bird_sound_wave('Snow Bunting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caspian Tern\n",
    "![](https://i.pinimg.com/originals/09/b5/0b/09b50b4dce31e02d1f93df92c0079984.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bird_sound_wave('Caspian Tern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barn Swallow\n",
    "![](https://www.allaboutbirds.org/guide/assets/photo/68123021-480px.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bird_sound_wave('Barn Swallow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Visualizing Audio in 2D</font><a id='32'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/birdsong-recognition/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../input/birdsong-recognition/train_audio/\"\n",
    "birds=train.ebird_code.unique()[:20]\n",
    "file=train[train.ebird_code==birds[0]]['filename'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20 ))\n",
    "for i in range(0,20):\n",
    "    file=train[train.ebird_code==birds[i]]['filename'].values[0]\n",
    "    audio_path=os.path.join(path,birds[i],file)\n",
    "    plt.subplot(20,1,i+1)\n",
    "    x , sr = librosa.load(audio_path)\n",
    "    librosa.display.waveplot(x, sr=sr)\n",
    "    plt.gca().set_title(birds[i])\n",
    "    plt.gca().get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Spectrogram Analysis</font><a id='33'></a>\n",
    "\n",
    "![](https://www.researchgate.net/profile/Phillip_Lobel/publication/267827408/figure/fig2/AS:295457826852866@1447454043380/Spectrograms-and-Oscillograms-This-is-an-oscillogram-and-spectrogram-of-the-boatwhistle.png)\n",
    "\n",
    "**What is a spectrogram?**\n",
    "A spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform.  Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time.  In other sciences spectrograms are commonly used to display frequencies of sound waves produced by humans, machinery, animals, whales, jets, etc., as recorded by microphones.  In the seismic world, spectrograms are increasingly being used to look at frequency content of continuous signals recorded by individual or groups of seismometers to help distinguish and characterize different types of earthquakes or other vibrations in the earth. \n",
    "\n",
    "**How do you read a spectrogram?**\n",
    "\n",
    "Spectrograms are basically two-dimensional graphs, with a third dimension represented by colors. Time runs from left (oldest) to right (youngest) along the horizontal axis. Each of our volcano and earthquake sub-groups of spectrograms shows 10 minutes of data with the tic marks along the horizontal axis corresponding to 1-minute intervals.  The vertical axis represents frequency, which can also be thought of as pitch or tone, with the lowest frequencies at the bottom and the highest frequencies at the top.  The amplitude (or energy or “loudness”) of a particular frequency at a particular time is represented by the third dimension, color, with dark blues corresponding to low amplitudes and brighter colors up through red corresponding to progressively stronger (or louder) amplitudes.\n",
    "![](https://s3.amazonaws.com/pnsn-cms-uploads/attachments/000/000/583/original/6dd1240572ba9085af145892a1b4c1eacce3a651)\n",
    "Above the spectrogram is the raw seismogram, drawn using the same horizontal time axis as the spectrogram (including the same tick marks), with the vertical axis representing wave amplitude. This plot is analogous to webicorder-style plots (or seismograms) that can be accessed via other parts of our website.  Collectively, the spectrogram-seismogram combination is a very powerful visualization tool, as it allows you to see raw waveforms for individual events and also the strength or “loudness” at various frequencies. The frequency content of an event can be very important in determining what produced the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "for i in range(0,5):\n",
    "    file=train[train.ebird_code==birds[i]]['filename'].values[0]\n",
    "    audio_path=os.path.join(path,birds[i],file)\n",
    "    plt.subplot(5,1,i+1)\n",
    "    x , sr = librosa.load(audio_path)\n",
    "    x = librosa.stft(x)\n",
    "    Xdb = librosa.amplitude_to_db(abs(x))\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.gca().set_title(birds[i])\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Audio Features<a id='4'></a>\n",
    "## <font size='4' color='blue'>Spectral Centroid</font><a id='41'></a>\n",
    "The spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located. Perceptually, it has a robust connection with the impression of brightness of a sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Spectral Bandwidth</font><a id='42'></a>\n",
    "The spectral bandwidth is defined as the width of the band of light at one-half the peak maximum (or full width at half maximum [FWHM]) and is represented by the two vertical red lines and λSB on the wavelength axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Spectral Rolloff</font><a id='43'></a>\n",
    "A feature extractor that extracts the Spectral Rolloff Point. This is a measure measure of the amount of the right-skewedness of the power spectrum.\n",
    "The spectral rolloff point is the fraction of bins in the power spectrum at which 85% of the power is at lower frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Zero-Crossing Rate</font><a id='44'></a>\n",
    "By looking at different speech and audio waveforms, we can see that depending on the content, they vary a lot in their smoothness. For example, voiced speech sounds are more smooth than unvoiced ones. Smoothness is thus a informative characteristic of the signal.\n",
    "\n",
    "A very simple way for measuring smoothness of a signal is to calculate the number of zero-crossing within a segment of that signal. A voice signal oscillates slowly - for example, a 100 Hz signal will cross zero 100 per second - whereas an unvoiced fricative can have 3000 zero crossing per second.\n",
    "\n",
    "To calculate of the zero-crossing rate of a signal you need to compare the sign of each pair of consecutive samples. In other words, for a length N signal you need O(N) operations. Such calculations are also extremely simple to implement, which makes the zero-crossing rate an attractive measure for low-complexity applications. However, there are also many drawbacks with the zero-crossing rate:\n",
    "\n",
    "The number of zero-crossings in a segment is an integer number. A continuous-valued measure would allow more detailed analysis.\n",
    "Measure is applicable only on longer segments of the signal, since short segments might not have any or just a few zero crossings.\n",
    "To make the measure consistent, we must assume that the signal is zero-mean. You should therefore subtract the mean of each segment before calculating the zero-crossings rate.\n",
    "An alternative to the zero-crossing rate is to calculate the autocorrelation at lag-1. It can be estimated also from short segments, it is continuous-valued and arithmetic complexity is also O(N).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Mel-Frequency Cepstral Coefficients(MFCCs)</font><a id='45'></a>\n",
    "\n",
    "**Mel Frequency Cepstral Coefficient (MFCC) tutorial**\n",
    "The first step in any automatic speech recognition system is to extract features i.e. identify the components of the audio signal that are good for identifying the linguistic content and discarding all the other stuff which carries information like background noise, emotion etc.\n",
    "\n",
    "The main point to understand about speech is that the sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. This page will provide a short tutorial on MFCCs.\n",
    "\n",
    "Mel Frequency Cepstral Coefficents (MFCCs) are a feature widely used in automatic speech and speaker recognition. They were introduced by Davis and Mermelstein in the 1980's, and have been state-of-the-art ever since. Prior to the introduction of MFCCs, Linear Prediction Coefficients (LPCs) and Linear Prediction Cepstral Coefficients (LPCCs) (click here for a tutorial on cepstrum and LPCCs) and were the main feature type for automatic speech recognition (ASR), especially with HMM classifiers. This page will go over the main aspects of MFCCs, why they make a good feature for ASR, and how to implement them.\n",
    "\n",
    "Steps at a Glance \n",
    "We will give a high level intro to the implementation steps, then go in depth why we do the things we do. Towards the end we will go into a more detailed description of how to calculate MFCCs.\n",
    "\n",
    "* Frame the signal into short frames.\n",
    "* For each frame calculate the periodogram estimate of the power spectrum.\n",
    "* Apply the mel filterbank to the power spectra, sum the energy in each filter.\n",
    "* Take the logarithm of all filterbank energies.\n",
    "* Take the DCT of the log filterbank energies.\n",
    "* Keep DCT coefficients 2-13, discard the rest.\n",
    "* There are a few more things commonly done, sometimes the frame energy is appended to each feature vector. Delta and Delta-Delta features are usually also appended. Liftering is also commonly applied to the final features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size='4' color='blue'>Chroma feature</font><a id='46'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Compare Sound Features<a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birdcall recordings Music - EDA \n",
    "![](https://storage.googleapis.com/pr-newsroom-wp/1/2020/03/Header.png)\n",
    "In continuation of previous kernel about spotify music data extraction -Part 1 \n",
    "https://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1\n",
    "\n",
    "We now will use the data extracted from Birdcall recordings to perform two steps as follows\n",
    "\n",
    "#### 1. Explore the Audio Features and analyze\n",
    "#### 2. Build a Machine Learning Model \n",
    "\n",
    "## 1. Explore the Audio Features and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "%matplotlib inline\n",
    "import pandas_profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us first analyse at high level the data in the spotify music dataframe that we build by accessing the spotify data as shown in part 1 of this kernel https://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add few more dataframes available datasets in kaggle for our deeper analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Important Note**: Considered only those columns which are related to audio features as follows :\n",
    "\n",
    "**Acousticness :** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "\n",
    "**Danceability** : Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "\n",
    "**Energy** : Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "\n",
    "**Instrumentalness**: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "\n",
    "**Liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
    "\n",
    "**Loudness**: he overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n",
    "\n",
    "**Speechiness**: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
    "\n",
    "**Valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "\n",
    "**Tempo**: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame with features\n",
    "def features(df,who):\n",
    "    if who == 1:\n",
    "         features = df.loc[: ,['acousticness', 'danceability','energy','instrumentalness','liveness', 'loudness','speechiness', 'tempo','valence']]         \n",
    "    elif who == 0 :   \n",
    "          features = df.loc[:,['acousticness', 'danceability', 'energy', 'instrumentalness','liveness', 'loudness', 'speechiness', 'tempo', 'valence','popularity']]           \n",
    "    else:\n",
    "        return 'Error'\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let create a dictionary in which the keys are the artists of both dataframes and the values are the total of songs for each singer or group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the data:\n",
    "We will plot a Bar chart and a Radar Chart showing the means of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation of the audio features themselves do not give us much information ( as we can see in the plots below), we can sum them up and calculate the mean of the standard deviation of the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Correlation Between Variables\n",
    "\n",
    "We will correlate the feature **valence** which describes the musical positiveness with **danceability** and **energy**.\n",
    "\n",
    "\n",
    "#### Valence and Energy\n",
    "The correlation between valence and energy shows us that there is a conglomeration of songs with high energy and a low level of valence. This means that many of my energetic songs sound more negative with feelings of sadness, anger and depression ( NF takes special place here haha). whereas when we look at the grays dots we can see that as the level of valence - positive feelings increase, the energy of the songs also increases. Although her data is split , we can identify this pattern which indicates a kind of 'linear' correlation between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valence and Danceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Machine Learning Approach\n",
    "I will be using different algorithms as I improve this kernel notebook to improve the model accuracy.So please keep watching this space on a frequent basis.\n",
    "\n",
    "Removing Features\n",
    "The first step is to preprocess our data set in order to have a dataframe with numerical values in all of the columns. So let's start off dropping all features which are not relevant to our model such as id, album, name, uri, popularity and track_number and separate the target from other artist dataframe. We can easily do that by building the function feature_elimination which receives a list with the features we want to drop as a parameter.\n",
    "\n",
    "Notice that after its removal, we still have a categorical feature (artist). So, we'll have to deal with that in the second step. Also, important to mention that we have two slightly balanced classes which indicate whose list the song belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us observe how the data is ? Is it balanced or not .Let us see.\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is well balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoder\n",
    "The second task is to transform all categocal data (artists names) into numeric data. Why do we have to do that? Well, the ML algorithm only accepts numerical data, hence, the reason why we have to use the class LabelEncoder to encode each artist name into a specific number. The encoding process is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted for Birdcall Dataset\n",
    "# Previous dataset removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6135ad1b7f034266e1f6db3633b021889ea3a27"
   },
   "source": [
    "# URBAN Sound CLASSIFICATION\n",
    "\n",
    "\n",
    "### Introduction\n",
    "When we get started with data science, we start with simple projects like Loan Prediction problem or Big Mart Sales Prediction. These problems have structured data arranged neatly in a tabular format i.e we are spoon-fed the hardest part in data science pipeline.The datasets in real life are much more complex and unstructured format like audio/image, collect it from various sources and arrange it in a format which is ready for processing. \n",
    "\n",
    "\n",
    "I have choosen an unstructured data as this problem of bird call classification as it represents huge under-exploited opportunity. It is closer to how we communicate and interact as humans. It also contains a lot of useful & powerful information. For example, if a person speaks; you not only get what he / she says but also what were the emotions of the person from the voice.Also the body language of the person can show you many more features about a person, because actions speak louder than words! So in short, unstructured data is complex but processing it can reap easy rewards.\n",
    "\n",
    "\n",
    "#### So what is audio data really mean ? \n",
    "\n",
    "Lets understand this with some theory before we actually jump in the real problem and its solution.\n",
    "\n",
    "Directly or indirectly, you are always in contact with audio. Your brain is continuously processing and understanding audio data and giving you information about the environment. A simple example can be your conversations with people which you do daily. This speech is discerned by the other person to carry on the discussions. Even when you think you are in a quiet environment, you tend to catch much more subtle sounds, like the rustling of leaves or the splatter of rain. This is the extent of your connection with audio.\n",
    "\n",
    "So in order to catch this audio floating around us there are devices which record in computer readable format. Examples of these formats are\n",
    "\n",
    "- wav (Waveform Audio File) format\n",
    "- mp3 (MPEG-1 Audio Layer 3) format\n",
    "- WMA (Windows Media Audio) format\n",
    "\n",
    "Audio typically looks like a wave like format of data, where the amplitude of audio change with respect to time. This can be pictorial represented as follows.\n",
    "\n",
    "![](sound.png)\n",
    "\n",
    "\n",
    "Real Time Applications of Audio Processing include but not limited\n",
    "\n",
    "- Indexing music collections according to their audio features.\n",
    "- Recommending music for radio channels\n",
    "- Similarity search for audio files (aka Shazam)\n",
    "- Speech processing and synthesis – generating artificial voice for conversational agents \n",
    "\n",
    "#### Data Handling in audio domain\n",
    "\n",
    "Audio data has a couple of preprocessing steps which have to be followed namely,\n",
    "\n",
    "- Firstly Load the data into a machine understandable format. \n",
    "    For this, we simply take values after every specific time steps. For example; in a 2 second audio file, we extract values at half a second. This is called ***sampling of audio data***, and the rate at which it is sampled is called the ***sampling rate***.\n",
    "    In this approach we have disadvantage i.e  When we sample an audio data, we require much more data points to represent the whole data and also, the sampling rate should be as high as possible.To offset this we can look at second approach.\n",
    "\n",
    "- The second approach of representing audio data is by converting it into a different domain of data representation, namely the ***frequency domain*** which require lesser computational space is required. . \n",
    "\n",
    "Now let us get more idea on this in detail\n",
    "\n",
    "![](time_freq.png)\n",
    "\n",
    "Here, we separate one audio signal into 3 different pure signals, which can now be represented as three unique values in frequency domain.\n",
    "\n",
    "There are a few more ways in which audio data can be represented, for example. using MFCs (Mel-Frequency cepstrums. PS: We will cover this in the later article). These are nothing but different ways to represent the data.\n",
    "\n",
    "Now the next step is to extract features from this audio representations, so that our algorithm can work on these features and perform the task it is designed for. Here’s a visual representation of the categories of audio features that can be extracted.\n",
    "\n",
    "![](audio-features.png)\n",
    "\n",
    "\n",
    "After extracting these features, it is then sent to the machine learning model for further analysis.\n",
    "\n",
    "Now enough theory.Lets jump into solving the Urban Sound Classifcation Problem\n",
    "\n",
    "### Objective\n",
    "\n",
    "The automatic classification of environmental sound is a growing research field with multiple applications to largescale, content-based multimedia indexing and retrieval. In particular, the sonic analysis of urban environments is the subject of increased interest, partly enabled by multimedia sensor networks, as well as by large quantities of online multimedia content depicting urban scenes.\n",
    "\n",
    "However, while there is a large body of research in related areas such as speech, music and bioacoustics, work on the analysis of urban acoustic environments is relatively scarce.Furthermore, when existent, it mostly focuses on the classification of auditory scene type, e.g. street, park, as opposed to the identification of sound sources in those scenes, e.g.car horn, engine idling, bird tweet. \n",
    "\n",
    "\n",
    "\n",
    "There are primarily two major challenges with bird call research namely\n",
    "\n",
    "- Lack of labeled audio data. Previous work has focused on audio from carefully produced movies or television tracks from specific environments such as elevators or office spaces and on commercial or proprietary datasets . The large effort involved in manually annotating real-world data means datasets based on field recordings tend to be relatively small (e.g. the event detection dataset of the IEEE AASP Challenge consists of 24 recordings per each of 17 classes).\n",
    "\n",
    "- Lack of common vocabulary when working on bird calls.This means the classification of sounds into semantic groups may vary from study to study, making it hard to compare results\n",
    "\n",
    "so the objective of this notebook is to address the above two mentioned challenges.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "The dataset is called UrbanSound and contains 8732 labeled sound excerpts (<=4s) of bird calls from 10 classes: -\n",
    "The dataset contains 8732 sound excerpts (<=4s) of bird calls from 10 classes, namely:\n",
    "\n",
    "- Air Conditioner\n",
    "- Car Horn\n",
    "- Children Playing\n",
    "- Dog bark\n",
    "- Drilling\n",
    "- Engine Idling\n",
    "- Gun Shot\n",
    "- Jackhammer\n",
    "- Siren\n",
    "- Street Music\n",
    "\n",
    "The attributes of data are as follows:\n",
    "\n",
    "ID – Unique ID of sound excerpt\n",
    "\n",
    "Class – type of sound\n",
    "\n",
    "The evaluation metric for this problem is \"Accuracy Score\"\n",
    "\n",
    "#### Source\n",
    "\n",
    "- Source of the dataset : https://drive.google.com/drive/folders/0By0bAi7hOBAFUHVXd1JCN3MwTEU\n",
    "- Source of research document : https://serv.cusp.nyu.edu/projects/urbansounddataset/salamon_urbansound_acmmm14.pdf\n",
    "\n",
    "\n",
    "Now let me look at a glance a sample sound excerpt from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4bcec54397111dc01d2ad8996be458a5a5ece7c"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio('../input/ultrasound-dataset/train/Train/2022.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f29de04532240585cb5818c7e39b0b21d35e0934"
   },
   "source": [
    "To load the audio files into the jupyter notebook ass a numpy array I have used 'librosa' library in python by using the pip command as follows\n",
    "\n",
    " ***pip install librosa***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "%pylab inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "91f98dc661b1d87f9d6ba9c4aa7dff30c88e5303"
   },
   "source": [
    "Now let us load a sample audio file using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8306846e5e9bf0e3d9ee4ac4094d1dd6469e1e9"
   },
   "outputs": [],
   "source": [
    "data,sampling_rate = librosa.load('../input/ultrasound-dataset/train/Train/2010.wav')\n",
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.waveplot(data,sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d44e64d3c5365cbd5462187c4c2a20c35059beb"
   },
   "source": [
    "Now let us visually inspect data and see if we can find patterns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55f24ebdec94e87f9351a312de8951f5a991c29a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/ultrasound-dataset/train/train.csv')\n",
    "i = random.choice(train.index)\n",
    "\n",
    "audio_name = train.ID[i]\n",
    "path = os.path.join('../input/ultrasound-dataset/train/', 'Train', str(audio_name) + '.wav')\n",
    "\n",
    "print('Class: ', train.Class[i])\n",
    "x, sr = librosa.load('../input/ultrasound-dataset/train/Train/' + str(train.ID[i]) + '.wav')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "225885357b4a7b5568ab9b28c2661e74e2936e2a"
   },
   "source": [
    "As you can see the air conditioner class is shown as random class and we can see its pattern.Let us again see another class by using the same code to randomly select another class and observe its pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "789325dcfde1a02bea2e451faffd7ac303b5c13f"
   },
   "outputs": [],
   "source": [
    "i = random.choice(train.index)\n",
    "audio_name = train.ID[i]\n",
    "path = os.path.join('../input/ultrasound-dataset/train/', 'Train', str(audio_name) + '.wav')\n",
    "print('Class: ', train.Class[i])\n",
    "x, sr = librosa.load('../input/ultrasound-dataset/train/Train/' + str(train.ID[i]) + '.wav')\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df1042ccc44a8ef8a0dd46593b9cc9abd99e51ba"
   },
   "source": [
    "Let us see the class distributions for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "593203a5c675dcb7050a1cdfa5eb7ed9d0cceec0"
   },
   "outputs": [],
   "source": [
    "print(train.Class.value_counts(normalize=True)) #distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a5edc0e3d276f18cd7e8162657bc0cda878bb6a"
   },
   "source": [
    "It appears that jackhammer has more count than any other classes\n",
    "\n",
    "Now let us see how we can leverage the concepts we learned above to solve the problem. We will follow these steps to solve the problem.\n",
    "\n",
    "- Step 1: Load audio files & Extract features\n",
    "- Step 2: Convert the data to pass it in our deep learning model\n",
    "- Step 3: Run a deep learning model and get results\n",
    "\n",
    "#### Step 1: Load audio files & Extract features\n",
    "\n",
    "Let us create a function to load audio files and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cd0d5eb913efd2dd5a9fdbdca876e721511cb58"
   },
   "outputs": [],
   "source": [
    "def parser(row):\n",
    "    file_name = os.path.join(os.path.abspath('../input/ultrasound-dataset/train/'),'Train',str(row.ID)+'.wav')\n",
    "    try:\n",
    "        # here kaiser_fast is a technique used for faster extraction\n",
    "        X,sample_rate = librosa.load(file_name,res_type='kaiser_fast')\n",
    "        # we extract mfcc feature from data\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate,n_mfcc=40).T,axis=0)\n",
    "    except Exception as e:\n",
    "        print('Error encountered while parsing the file:',file_name)\n",
    "        \n",
    "        return 'None', 'None'\n",
    "    \n",
    "    feature = mfccs\n",
    "    \n",
    "    label = row.Class\n",
    "    #print(file_name)\n",
    "    print(feature)\n",
    "    print(label)\n",
    "    return pd.Series([feature, label],index=['feature','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "786b607c60748d35391d9d6692867b5d98c31740"
   },
   "outputs": [],
   "source": [
    "temp = train.apply(parser,axis =1)\n",
    "temp.columns = ['feature', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "433d28c32afb1ae9c3e0c8b030ca883671e28fcc"
   },
   "source": [
    "#### Step 2: Convert the data to pass it in our deep learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f70092010147fe50a5ccdc6e6aad7a3df32138b4"
   },
   "outputs": [],
   "source": [
    "X = np.array(temp.feature.tolist())\n",
    "y = np.array(temp.label.tolist())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "print(temp.label.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8da4d73ed70585adeb322bb47e5b0bcb53f9917b"
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(label_encoder.fit_transform(y))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you like this kernel greatly appreciate to <font color='red'>UPVOTE</font>."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1292430,
     "sourceId": 19596,
     "sourceType": "competition"
    },
    {
     "datasetId": 1833,
     "sourceId": 3172,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 655608,
     "sourceId": 1158683,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 673455,
     "sourceId": 1184794,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
